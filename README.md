# ðŸš¦ Multi-Agent Reinforcement Learning for Smart Traffic Signal Control

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A state-of-the-art implementation of **QMIX** algorithm for optimizing traffic signal control in urban networks using Multi-Agent Reinforcement Learning.

## ðŸŽ¯ Features

- âœ… **QMIX Algorithm**: Value decomposition with mixing networks
- âœ… **Multi-Intersection Environment**: 2Ã—2 grid with 4 coordinated traffic lights
- âœ… **Advanced Neural Networks**: GRU-based Q-networks for partial observability
- âœ… **Realistic Traffic Simulation**: Dynamic traffic patterns with rush hour modeling
- âœ… **Comprehensive Metrics**: Waiting time, queue length, throughput tracking
- âœ… **Experience Replay**: Stable training with replay buffer
- âœ… **Model Checkpointing**: Save best and periodic models

## ðŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/marl-traffic-control.git
cd marl-traffic-control

# Install dependencies
pip install -e .
pip install torch torchvision torchaudio

# Verify installation
python -c "import torch; print('âœ“ Ready to train!')"


Training
Bash

# Train QMIX (500 episodes, ~15-20 minutes)
python scripts/train_qmix.py

# Quick test (5 episodes, ~30 seconds)
# Edit configs/qmix_config.yaml: total_episodes: 5
python scripts/train_qmix.py


ðŸŽ“ Algorithm: QMIX
QMIX (Monotonic Value Function Factorisation) enables centralized training with decentralized execution:

Key Components
Agent Networks: GRU-based Q-networks (one shared network for all agents)

Input: Local observation (queue lengths, waiting times, phase info)
Output: Q-values for 4 traffic light phases
Mixing Network: Combines individual Q-values into joint Q-value

Hypernetworks generate mixing weights from global state
Monotonicity constraint ensures alignment
Training:

Centralized: Uses global state and all agent observations
Execution: Each agent acts based on local observation only


Architecture
text

Individual Agents â†’ [GRU Q-Networks] â†’ Individual Q-values
                                              â†“
Global State â†’ [Hypernetwork] â†’ Mixing Weights
                                              â†“
                                    [Mixer] â†’ Total Q-value



ðŸ“ˆ Results
Training Metrics
After training, you'll find:

Bash

# Saved models
checkpoints/qmix/
â”œâ”€â”€ qmix_best.pth      # Best performing model
â”œâ”€â”€ qmix_final.pth     # Final model
â””â”€â”€ qmix_ep*.pth       # Periodic checkpoints

# Training metrics (JSON)
results/qmix_metrics.json
Expected Performance
Average Reward: ~-150 to -160 (lower is better - less congestion)
Queue Length: ~2.3 vehicles per lane
Waiting Time: ~45 seconds average
Throughput: ~450 vehicles per episode
Sample Output
text

Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [16:23<00:00]

ðŸ“Š Final Statistics (last 100 episodes):
   Average Reward: -156.82
   Average Queue Length: 2.34
   Average Waiting Time: 45.67s
   Average Throughput: 452 vehicles


   ðŸ§ª Testing
Bash

# Quick test (5 episodes)
python -c "
import yaml
with open('configs/qmix_config.yaml', 'r') as f:
    config = yaml.safe_load(f)
config['training']['total_episodes'] = 5
with open('configs/qmix_config.yaml', 'w') as f:
    yaml.dump(config, f)
"
python scripts/train_qmix.py


ðŸŽ¯ Key Features Explained
1. Partial Observability
Each intersection observes:

Own queue lengths (N, S, E, W)
Own waiting times
Current phase and phase duration
Neighbor queue information
2. Coordination
Agents coordinate through:

Shared experiences in replay buffer
Mixing network that combines Q-values
Information about neighboring intersections
3. Dynamic Traffic
Poisson arrival process
Rush hour simulation (7-9 AM, 5-7 PM)
Adaptive green time based on demand
ðŸ—ï¸ Requirements
Python 3.8+
PyTorch 2.0+
NumPy
PyYAML
tqdm
Other dependencies in requirements.txt
ðŸ“š References
QMIX Paper: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning (Rashid et al., 2018)
Multi-Agent RL: An Introduction to Multi-Agent RL


â­ If you find this project useful, please give it a star!!!

